{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aad22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The estimation of the MLE is based on the material from Quantecon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b0eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need the following imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86d351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #set default figure size\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "from scipy.special import factorial\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import Poisson\n",
    "from statsmodels.api import Probit\n",
    "from statsmodels.api import Logit\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.iolib.summary2 import summary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8aa090c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>\n",
       "Consider the probit model where the dependent variable ($Y$) is binary, and the independent variables are given by the vector $X$. \n",
       "In addition, assume that $Pr(Y=1|X)= Φ(X^T β)$ where $Φ$ is the CDF of the standard normal distribution. \n",
       "</p>\n",
       "\n",
       "<p>\n",
       "The objective is to maximize the likelihood function over the parameters to find the Maximum Likelihood Estimator (MLE). \n",
       "To find the MLE estimator we maximize the next Log-Likelihood function:\n",
       "</p>\n",
       "\n",
       "<p>\n",
       "$$\n",
       "\\ln L(\\beta \\mid X, Y)=\\sum_{i=1}^n\\left[y_i \\ln \\Phi\\left(x_i^{\\prime} \\beta\\right)+\\left(1-y_i\\right) \\ln \\left(1-\\Phi\\left(x_i^{\\prime} \\beta\\right)\\right)\\right]\n",
       "$$\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<p>\n",
    "Consider the probit model where the dependent variable ($Y$) is binary, and the independent variables are given by the vector $X$. \n",
    "In addition, assume that $Pr(Y=1|X)= Φ(X^T β)$ where $Φ$ is the CDF of the standard normal distribution. \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The objective is to maximize the likelihood function over the parameters to find the Maximum Likelihood Estimator (MLE). \n",
    "To find the MLE estimator we maximize the next Log-Likelihood function:\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "$$\n",
    "\\ln L(\\beta \\mid X, Y)=\\sum_{i=1}^n\\left[y_i \\ln \\Phi\\left(x_i^{\\prime} \\beta\\right)+\\left(1-y_i\\right) \\ln \\left(1-\\Phi\\left(x_i^{\\prime} \\beta\\right)\\right)\\right]\n",
    "$$\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97f046cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Profit Log-Likelihood Function from https://nbviewer.org/github/VHRanger/MLE-tutorial/blob/master/Implementing%20and%20vectorizing%20a%20Maximum%20Likelihood%20model%20with%20scipy--1.ipynb\n",
    "\n",
    "def LogLikeProbit(betas, y, x):\n",
    "    \"\"\"\n",
    "    Probit Log Likelihood function\n",
    "    Very slow naive Python version\n",
    "    Input:\n",
    "        betas is a np.array of parameters\n",
    "        y is a one dimensional np.array of endogenous data\n",
    "        x is a 2 dimensional np.array of exogenous data\n",
    "            First vertical column of X is assumed to be constant term,\n",
    "            corresponding to betas[0]\n",
    "    returns:\n",
    "        negative of log likehood value (scalar)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    #Sum operation\n",
    "    for i in range(0, len(y)):\n",
    "        #Get X'_i * Beta value\n",
    "        xb = np.dot(x[i], betas)\n",
    "        \n",
    "        #compute both binary probabilities from xb     \n",
    "        #Add to total log likelihood\n",
    "        llf = y[i]*np.log(norm.cdf(xb)) + (1-y[i])*np.log(1 - norm.cdf(xb))\n",
    "        result += llf\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71894329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#ARTIFICIAL DATA\n",
    "######################\n",
    "\n",
    "#sample size\n",
    "n = 1000\n",
    "\n",
    "#random generators\n",
    "z1 = np.random.randn(n)\n",
    "z2 = np.random.randn(n)\n",
    "\n",
    "#create artificial exogenous variables \n",
    "x1 = 0.8*z1 + 0.2*z2\n",
    "x2 = 0.2*z1 + 0.8*z2\n",
    "#create error term\n",
    "u = 2*np.random.randn(n)\n",
    "\n",
    "#create endogenous variable from x1, x2 and u\n",
    "ystar = 0.5 + 0.75*x1 - 0.75*x2 + u\n",
    "\n",
    "#create latent binary variable from ystar\n",
    "def create_dummy(data, cutoff):\n",
    "    result = np.zeros(len(data))\n",
    "    for i in range(0, len(data)):\n",
    "        if data[i] >= cutoff:\n",
    "            result[i] = 1\n",
    "        else:\n",
    "            result[i] = 0\n",
    "    return result\n",
    "\n",
    "#get latent LHS variable\n",
    "y = create_dummy(ystar, 0.5)\n",
    "\n",
    "#prepend vector of ones to RHS variables matrix\n",
    "#for constant term\n",
    "const = np.ones(n)\n",
    "x = np.column_stack((const, np.column_stack((x1, x2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "905fb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1886f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02607502,  0.41013366, -0.36970816])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create beta hat vector to maximize on\n",
    "#will store the values of maximum likelihood beta parameters\n",
    "#Arbitrarily initialized to all zeros\n",
    "bhat = np.zeros(len(x[0]))\n",
    "\n",
    "#unvectorized MLE estimation\n",
    "probit_est = minimize(LogLikeProbit, bhat, args=(y,x), method='nelder-mead')\n",
    "\n",
    "#print vector of maximized betahats\n",
    "probit_est['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4daa641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta Hats:  [ 0.02607502  0.41013366 -0.36970816]\n",
      "SE:  [0.04046088 0.05752986 0.0550724 ]\n",
      "t stat:  [ 0.64445017  7.12905693 -6.71313008]\n",
      "P value:  [5.19431501e-01 1.93558832e-12 3.18915601e-11]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.tools.numdiff as smt\n",
    "import scipy as sc\n",
    "\n",
    "#Get inverse hessian for Cramer Rao lower bound\n",
    "b_estimates = probit_est['x']\n",
    "Hessian = smt.approx_hess3(b_estimates, LogLikeProbit, args=(y,x))\n",
    "invHessian = np.linalg.inv(Hessian)\n",
    "\n",
    "#Standard Errors from C-R LB\n",
    "#from diagonal elements of invHessian\n",
    "SE = np.zeros(len(invHessian))\n",
    "for i in range(0, len(invHessian)):\n",
    "    SE[i] =  np.sqrt(invHessian[i,i])\n",
    "    \n",
    "#t and p values\n",
    "t_statistics = (b_estimates/SE)\n",
    "pval = (sc.stats.t.sf(np.abs(t_statistics), 999)*2)\n",
    "\n",
    "print(\"Beta Hats: \", b_estimates)\n",
    "print(\"SE: \", SE)\n",
    "print(\"t stat: \", t_statistics)\n",
    "print(\"P value: \", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea7c2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.659063\n",
      "         Iterations 4\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 1000\n",
      "Model:                         Probit   Df Residuals:                      997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 18 Oct 2022   Pseudo R-squ.:                 0.04884\n",
      "Time:                        23:09:23   Log-Likelihood:                -659.06\n",
      "converged:                       True   LL-Null:                       -692.91\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.007e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0261      0.040      0.644      0.519      -0.053       0.105\n",
      "x1             0.4102      0.058      7.129      0.000       0.297       0.523\n",
      "x2            -0.3697      0.055     -6.713      0.000      -0.478      -0.262\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels \n",
    "#Using other routine\n",
    "stats_probit = Probit(y, x).fit()\n",
    "print(stats_probit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd20c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
