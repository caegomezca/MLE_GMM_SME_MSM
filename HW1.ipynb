{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d793163",
   "metadata": {},
   "source": [
    "\n",
    "### The estimation of the MLE is based on the material from Quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94c829",
   "metadata": {},
   "source": [
    "First, we need the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4078d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #set default figure size\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "from scipy.special import factorial\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import Poisson\n",
    "from statsmodels.api import Probit\n",
    "from statsmodels.api import Logit\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.iolib.summary2 import summary_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d5d57",
   "metadata": {},
   "source": [
    "Consider the probit model where the dependent variable ($Y$) is binary, and the independent variables are given by the vector $X$. \n",
    "In addition, assume that $Pr(Y=1|X)= Φ(X^T β)$ where $Φ$ is the CDF of the standard normal distribution. \n",
    "\n",
    "The objective is to maximize the likelihood function over the parameters to find the Maximum Likelihood Estimator (MLE). \n",
    "To find the MLE estimator we maximize the next Log-Likelihood function:\n",
    "\n",
    "$$\n",
    "\\ln L(\\beta \\mid X, Y)=\\sum_{i=1}^n\\left[y_i \\ln \\Phi\\left(x_i^{\\prime} \\beta\\right)+\\left(1-y_i\\right) \\ln \\left(1-\\Phi\\left(x_i^{\\prime} \\beta\\right)\\right)\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acde932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Profit Log-Likelihood Function from https://nbviewer.org/github/VHRanger/MLE-tutorial/blob/master/Implementing%20and%20vectorizing%20a%20Maximum%20Likelihood%20model%20with%20scipy--1.ipynb\n",
    "\n",
    "def LogLikeProbit(betas, y, x):\n",
    "    \"\"\"\n",
    "    Probit Log Likelihood function\n",
    "    Very slow naive Python version\n",
    "    Input:\n",
    "        betas is a np.array of parameters\n",
    "        y is a one dimensional np.array of endogenous data\n",
    "        x is a 2 dimensional np.array of exogenous data\n",
    "            First vertical column of X is assumed to be constant term,\n",
    "            corresponding to betas[0]\n",
    "    returns:\n",
    "        negative of log likehood value (scalar)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    #Sum operation\n",
    "    for i in range(0, len(y)):\n",
    "        #Get X'_i * Beta value\n",
    "        xb = np.dot(x[i], betas)\n",
    "        \n",
    "        #compute both binary probabilities from xb     \n",
    "        #Add to total log likelihood\n",
    "        llf = y[i]*np.log(norm.cdf(xb)) + (1-y[i])*np.log(1 - norm.cdf(xb))\n",
    "        result += llf\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039dbfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#ARTIFICIAL DATA\n",
    "######################\n",
    "\n",
    "#sample size\n",
    "n = 1000\n",
    "\n",
    "#random generators\n",
    "z1 = np.random.randn(n)\n",
    "z2 = np.random.randn(n)\n",
    "\n",
    "#create artificial exogenous variables \n",
    "x1 = 0.8*z1 + 0.2*z2\n",
    "x2 = 0.2*z1 + 0.8*z2\n",
    "#create error term\n",
    "u = 2*np.random.randn(n)\n",
    "\n",
    "#create endogenous variable from x1, x2 and u\n",
    "ystar = 0.5 + 0.75*x1 - 0.75*x2 + u\n",
    "\n",
    "#create latent binary variable from ystar\n",
    "def create_dummy(data, cutoff):\n",
    "    result = np.zeros(len(data))\n",
    "    for i in range(0, len(data)):\n",
    "        if data[i] >= cutoff:\n",
    "            result[i] = 1\n",
    "        else:\n",
    "            result[i] = 0\n",
    "    return result\n",
    "\n",
    "#get latent LHS variable\n",
    "y = create_dummy(ystar, 0.5)\n",
    "\n",
    "#prepend vector of ones to RHS variables matrix\n",
    "#for constant term\n",
    "const = np.ones(n)\n",
    "x = np.column_stack((const, np.column_stack((x1, x2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fd5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31e4037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.42615382e-04,  4.05914825e-01, -4.80176299e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create beta hat vector to maximize on\n",
    "#will store the values of maximum likelihood beta parameters\n",
    "#Arbitrarily initialized to all zeros\n",
    "bhat = np.zeros(len(x[0]))\n",
    "\n",
    "#unvectorized MLE estimation\n",
    "probit_est = minimize(LogLikeProbit, bhat, args=(y,x), method='nelder-mead')\n",
    "\n",
    "#print vector of maximized betahats\n",
    "probit_est['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4056644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta Hats:  [ 4.42615382e-04  4.05914825e-01 -4.80176299e-01]\n",
      "SE:  [0.04063554 0.05854282 0.05949959]\n",
      "t stat:  [ 0.01089232  6.93364024 -8.07024563]\n",
      "P value:  [9.91311532e-01 7.35300473e-12 2.00257040e-15]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.tools.numdiff as smt\n",
    "import scipy as sc\n",
    "\n",
    "#Get inverse hessian for Cramer Rao lower bound\n",
    "b_estimates = probit_est['x']\n",
    "Hessian = smt.approx_hess3(b_estimates, LogLikeProbit, args=(y,x))\n",
    "invHessian = np.linalg.inv(Hessian)\n",
    "\n",
    "#Standard Errors from C-R LB\n",
    "#from diagonal elements of invHessian\n",
    "SE = np.zeros(len(invHessian))\n",
    "for i in range(0, len(invHessian)):\n",
    "    SE[i] =  np.sqrt(invHessian[i,i])\n",
    "    \n",
    "#t and p values\n",
    "t_statistics = (b_estimates/SE)\n",
    "pval = (sc.stats.t.sf(np.abs(t_statistics), 999)*2)\n",
    "\n",
    "print(\"Beta Hats: \", b_estimates)\n",
    "print(\"SE: \", SE)\n",
    "print(\"t stat: \", t_statistics)\n",
    "print(\"P value: \", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4cd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.652665\n",
      "         Iterations 4\n",
      "                          Probit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 1000\n",
      "Model:                         Probit   Df Residuals:                      997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Wed, 19 Oct 2022   Pseudo R-squ.:                 0.05840\n",
      "Time:                        19:00:06   Log-Likelihood:                -652.67\n",
      "converged:                       True   LL-Null:                       -693.15\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.629e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0004      0.041      0.011      0.991      -0.079       0.080\n",
      "x1             0.4059      0.059      6.934      0.000       0.291       0.521\n",
      "x2            -0.4802      0.059     -8.070      0.000      -0.597      -0.364\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#Using other routine\n",
    "stats_probit = Probit(y, x).fit()\n",
    "print(stats_probit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974109f",
   "metadata": {},
   "source": [
    "### Maximum Simulated Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5303cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.49869379 -0.39043127]\n",
      " [ 1.          0.13793367  2.2682735 ]\n",
      " [ 1.         -0.21080265  0.02435377]\n",
      " ...\n",
      " [ 1.         -0.38827519  0.30214358]\n",
      " [ 1.         -1.54603649  0.40220462]\n",
      " [ 1.          0.7533242   1.08377725]]\n"
     ]
    }
   ],
   "source": [
    "#set the number of simulations\n",
    "S = 1000\n",
    "\n",
    "#Define the Random Parameter Mixed Simulated Likelihood Function.\n",
    "def mslf(θ, y, x):\n",
    "  y = y\n",
    "  x = x\n",
    "  print(x)\n",
    "\n",
    "mslf(2,y,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed462d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of simulations\n",
    "S = 1000\n",
    "\n",
    "#Define the Random Parameter Mixed Simulated Likelihood Function.\n",
    "def mslf(θ, y, x):\n",
    "  y = y\n",
    "  x = x\n",
    "    \n",
    "\n",
    "\n",
    "mslf <- function(param){\n",
    "  #Set up the major variables that will be used to created a likelihood\n",
    "  choice<-Data$preference_liking\n",
    "  endowment<-Data$InitialGood_Stage1\n",
    "  \n",
    "  #set of parameters we are hoping to find the MSL estimates of.\n",
    "  lambda_m<-param[1]\n",
    "  u1<-param[2]\n",
    "  u2<-param[3]\n",
    "  u3<-param[4]\n",
    "  u4<-param[5]\n",
    "  delt<-param[6]\n",
    "  sd<-exp(param[7])\n",
    "  \n",
    "\n",
    "  sim_avg_f=0\n",
    "  \n",
    "  set.seed(10101)\n",
    "  \n",
    "  #create the for loop over which we generate the simulated likelihood function\n",
    "  for(i in 1:num_draws){\n",
    "    #first, generate a set of random normal variables for each individual\n",
    "    #This will represent the underlying (unobserved) heterogeneity in our random parameter model.\n",
    "    unobserved_noise<-rnorm(nrow(Data), 0, 1)\n",
    "    \n",
    "    #Draw lambda value for an individual, sampling from the mean value (lambda_temp) with noise e*sd.\n",
    "    lambda<-lambda_m+unobserved_noise*sd\n",
    "    \n",
    "    #Given individual context, generate the KR structural utilities.\n",
    "    #Good a represents the endowment, so we compute U(a|a).\n",
    "    kr_utils_good_a=u1*(endowment==1)+u2*(endowment==2)+u3*(endowment==3)+u4*(endowment==4)\n",
    "    #Good b represents the alternative good, so we compute U(b|a)\n",
    "    kr_utils_good_b=(2*u2-lambda*u1)*(endowment==1)+(2*u1-lambda*u2)*(endowment==2)+\n",
    "                    (2*u4-lambda*u3)*(endowment==3)+(2*u3-lambda*u4)*(endowment==4)\n",
    "    \n",
    "    #Construct the likelihood at the given draw\n",
    "    sim_f=(exp(kr_utils_good_a)/(exp(kr_utils_good_a)+exp(kr_utils_good_b+delt)))*(choice==1)+\n",
    "          (exp(kr_utils_good_b)/(exp(kr_utils_good_b)+exp(kr_utils_good_a+delt)))*(choice==-1)+\n",
    "        (1- (exp(kr_utils_good_b)/(exp(kr_utils_good_b)+exp(kr_utils_good_a+delt)))-\n",
    "           (exp(kr_utils_good_a)/(exp(kr_utils_good_a)+exp(kr_utils_good_b+delt))) )*(choice==0)\n",
    "        \n",
    "    sim_avg_f = sim_avg_f + sim_f/num_draws\n",
    "\n",
    "  }\n",
    "  log(sim_avg_f)\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
